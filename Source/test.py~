# this script should be invoked on each compute node
# it collects the experiments on one dataset and save it to a separate place
# it takes in a list of numbers (e.g., 1,3,4) as the index ID of the datasets to process

import os

import shutil

import sys

from Methods import *
import numpy as np

def cleanWorkspace (pathUCRResult, distanceFileDir, datasetsNameFile, dataSizeFiles, maxdim, w):
    usableDataSets = []
    datasizes=[]
    sizes = []
#    a = input("Are you sure to clean the workspace in "+pathUCRResult+"? (y/n) ")
#    if a!='y':
#        print("Please rerun me when you are ready.")
#        exit()
    datasets = []
    with open(datasetsNameFile, 'r') as f:
        for line in f:
            datasets.append(line.strip())
    with open(dataSizeFiles, 'r') as f:
        for line in f:
            datasizes.append(line.strip())

    for idx, d in enumerate(datasets):
        datasetName = d
        if os.path.isdir(pathUCRResult + "/" + d):
            shutil.rmtree(pathUCRResult + "/" + d)
        newdir = pathUCRResult+'/'+ datasetName +'/d'+str(maxdim)+'/w'+str(w)
        os.makedirs(newdir)
        distanceFile = distanceFileDir+'/w='+str(w)+'/'+datasetName+"_DTWdistances.npy"
        if os.path.exists(distanceFile):
            usableDataSets.append(datasetName)
            newFile = newdir+'/0X0_NoLB_DTWdistances.npy'
            dists = np.load(distanceFile)
            np.save(newFile, dists)
            sizes.append(datasizes[idx])
        else:
            print('Dataset '+datasetName+' has no dtw distances.')
    if os.path.exists(pathUCRResult+'/_AllDataSets/'):
        shutil.rmtree(pathUCRResult+'/_AllDataSets/')
    os.makedirs(pathUCRResult+'/_AllDataSets/d'+str(maxdim))
    return usableDataSets, sizes

logfile = '../Results/experiment_arc.log'
argvs = sys.argv
args = len(argvs)
if (args<2):
    print('Please indicate which datasets to work on.')
    exit()
datasetsIDs = argvs[1].split(',')
datasetsIDs = [int(x) for x in datasetsIDs]

datapath="/home/xshen5/Research/TriangleDTW/Multivariate_pickled/"
#datapath="/Users/xshen/Kids/DanielShen/Research/DTW/Triangle/workshop/TriangleDTW/Data/Multivariate_pickled/"
distanceFileDir = "../Results/UCR_DTWDistances/"
alldatasetsNameFile = "../Results/UCR/workableDatasets.txt"
alldatasetsSizeFile = "../Results/UCR/workableDatasets_size.txt"

# set up the directory to hold the results
hostname = os.popen('/bin/hostname')
hostname = hostname.read().strip('\n')
orgpathUCRResult = "../Results/UCR/"+hostname
pathUCRResult = orgpathUCRResult
maxDirs = 20
for i in range(maxDirs):
    if os.path.exists(pathUCRResult):
        pathUCRResult = orgpathUCRResult+'_'+str(i)
    else:
        break
if (os.path.exists(pathUCRResult)):
    print('More than '+ str(maxDirs) +'directories were created by this node. Too many!')
    exit()
else:
    os.makedirs(pathUCRResult)
pathUCRResult=pathUCRResult+'/'

# create the datasetsNameFile and datasetsSizeFile
datasetsNameFile = pathUCRResult+"datasets_node"+hostname+"_allStages.txt"
datasetsSizeFile = pathUCRResult+"datasets_size_node"+hostname+"_allStages.txt"
with open(alldatasetsNameFile, 'r') as f:
    alldatasets = f.read().strip()
alldatasets = alldatasets.split('\n')
thisnodesets = [alldatasets[i-1] for i in datasetsIDs]
np.savetxt(datasetsNameFile, thisnodesets, fmt='%s')
with open(alldatasetsSizeFile, 'r') as f:
    alldatasizes = f.read().strip()
alldatasizes=alldatasizes.split('\n')
thisnodesizes = [alldatasizes[i-1] for i in datasetsIDs]
np.savetxt(datasetsSizeFile, thisnodesizes, fmt='%s')

with open(logfile, 'a+') as f:
    f.write('Node '+hostname+' : '+ ",".join(thisnodesets)+" . Full run.\n")
#datasetsSizeFile = pathUCRResult+"size_no_EigenWorms.txt"
#datasetsNameFile = pathUCRResult+"allDataSetsNames_firstTwo.txt"
#datasetsSizeFile = pathUCRResult+"size_firstTwo.txt"
#datasetsNameFile = pathUCRResult+"only2ndDataset.txt"
#datasetsSizeFile = pathUCRResult+"size_only2ndDataset.txt"

maxdim_g = 5
nqueries_g = 0
nreferences_g = 0
windows_g = [20]
machineRatios = [1, 1]
THs_g = [0.05, 0.1, 0.2]
#THs_g = [0.2, 0.4, 0.6, 0.8]
#Ks_g = [4, 6, 8]
Ks_g = [6]
Qs_g = [2, 3]
#THs_g_Z3 = [0.8, 0.5, 0.3, 0.1]
THs_g_Z3 = [0.1, 0.5]
K_g = 4
period_g = 5
#
# ##### Clean workspace
usableDataSets, sizes = cleanWorkspace(pathUCRResult, distanceFileDir, datasetsNameFile, datasetsSizeFile, maxdim_g, windows_g[0])
datasetsNameFile = pathUCRResult+'actuallyWorkedOnDatasets.txt'
datasetsSizeFile = pathUCRResult+'actuallyWorkedOnDatasets_size.txt'
with open(datasetsNameFile,'w') as f:
    [f.write(l+'\n') for l in usableDataSets]
with open(datasetsSizeFile, 'w') as f:
    [f.write(l+'\n') for l in sizes]


print(">>>>>> Measure prime times.")
MeasurePerformance.MeasurePrimeTimes \
    (pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath, maxdim_g, windows_g[0], Ks_g, Qs_g)

print(">>>>>> Start Z9")
Z9.dataCollection(pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath,maxdim_g,nqueries_g,nreferences_g,windows_g)

#
print(">>>>>> Start X0")
X0.dataCollection(pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath,maxdim_g,nqueries_g,nreferences_g,windows_g)
X0.dataProcessing(datasetsNameFile, pathUCRResult, maxdim_g, nqueries_g, nreferences_g, windows_g, machineRatios)
#
#
print(">>>>>> Start X0a")
X0a.dataCollection(pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath,maxdim_g,nqueries_g,nreferences_g,windows_g)
X0a.dataProcessing(datasetsNameFile, pathUCRResult, maxdim_g, nqueries_g, nreferences_g, windows_g, machineRatios)
#
# print(">>>>>> Start X1e")
# X1e.dataCollection(pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath, maxdim_g, nqueries_g, nreferences_g, windows_g, THs_g)
# X1e.dataProcessing(datasetsNameFile, pathUCRResult, maxdim_g, nqueries_g, nreferences_g, windows_g, THs_g)
#
print(">>>>>> Start X1ea")
X1ea.dataCollection(pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath, maxdim_g, nqueries_g, nreferences_g, windows_g, THs_g)
X1ea.dataProcessing(datasetsNameFile, pathUCRResult, maxdim_g, nqueries_g, nreferences_g, windows_g, THs_g)

print(">>>>>> Start X3rsea")
X3rsea.dataCollection(pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath, maxdim_g, nqueries_g, nreferences_g, windows_g, Ks_g, Qs_g)
X3rsea.dataProcessing(datasetsNameFile, pathUCRResult, maxdim_g, nqueries_g, nreferences_g, windows_g, Ks_g, Qs_g)

print(">>>>>> Start Z0a")
Z0a.dataCollection(pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath, maxdim_g, nqueries_g, nreferences_g, windows_g)
Z0a.dataProcessing(datasetsNameFile, pathUCRResult, maxdim_g, nqueries_g, nreferences_g, windows_g)

print(">>>>>> Start Z1ea")
Z1ea.dataCollection(pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath, maxdim_g, nqueries_g, nreferences_g, windows_g, THs_g)
Z1ea.dataProcessing(datasetsNameFile, pathUCRResult, maxdim_g, nqueries_g, nreferences_g, windows_g, THs_g)

print(">>>>>> Start Z3ea")
Z3ea.dataCollection(pathUCRResult, datasetsNameFile, datasetsSizeFile, datapath, maxdim_g, nqueries_g, nreferences_g, windows_g, Ks_g, Qs_g, THs_g_Z3)
Z3ea.dataProcessing(datasetsNameFile, pathUCRResult, maxdim_g, nqueries_g, nreferences_g, windows_g, Ks_g, Qs_g, THs_g_Z3)

print("End")
